{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_size=1000\n",
    "test_frac = 0.1\n",
    "input_size = 18\n",
    "output_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size: 2 sets of 8 subjects + 2 indicators for activities\n",
    "output_size: 8 subject grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoremaker():\n",
    "    #empty scores represent the subjects that the student doesn't take\n",
    "    emptyScores = np.random.randint(low = 0, high = output_size, size=(3))        \n",
    "    \n",
    "    #randomly generate 2 sets of results. For different schools, scale the result accordingly\n",
    "    scores = np.random.randint(low = 50, high = 90, size=(output_size))\n",
    "    scores = scores + np.random.randint(low = -10, high = 10, size=(output_size))\n",
    "    scores2 = np.random.randint(low = 50, high = 90, size=(output_size))\n",
    "    scores2 = scores2 + np.random.randint(low = -10, high = 10, size=(output_size))\n",
    "    \n",
    "    #generate 2 values, 1 for humanities-related activities and one for science activities. More activities = higher score\n",
    "    activities = np.random.randint(low = 0, high = 10, size = (2))\n",
    "    \n",
    "    #generate fake labels for each listing.\n",
    "    scoreLabels = (scores + scores2) / 2\n",
    "    \n",
    "    #add activity score to labels for each subject\n",
    "    for i in range(output_size):\n",
    "        if(i < 4): scoreLabels[i] += activities[0]\n",
    "        else: scoreLabels[i] += activities[1]\n",
    "\n",
    "    scoreLabels = (scoreLabels - 40) / 60\n",
    "    \n",
    "    #make subjects empty\n",
    "    for i in emptyScores:\n",
    "        scores[i] = 0\n",
    "        scores2[i] = 0\n",
    "        scoreLabels[i] = 0\n",
    "        \n",
    "    return np.concatenate((scores, scores2, activities, scoreLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([83.        , 64.        , 59.        , 76.        , 52.        ,\n",
       "        0.        ,  0.        , 85.        , 90.        , 78.        ,\n",
       "       54.        , 51.        , 64.        ,  0.        ,  0.        ,\n",
       "       84.        ,  1.        ,  4.        ,  0.79166667,  0.53333333,\n",
       "        0.29166667,  0.40833333,  0.36666667,  0.        ,  0.        ,\n",
       "        0.80833333])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoremaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.array([scoremaker() for i in range(test_data_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59.         80.         90.          0.         87.         78.\n",
      "  64.          0.         65.         77.         49.          0.\n",
      "  69.         75.         81.          0.          2.          3.\n",
      "   0.4         0.675       0.525       0.          0.68333333  0.65833333\n",
      "   0.59166667  0.        ]\n",
      " [69.          0.         59.         91.         85.          0.\n",
      "   0.         52.         88.          0.         62.         76.\n",
      "  76.          0.          0.         54.          7.          3.\n",
      "   0.75833333  0.          0.45833333  0.84166667  0.725       0.\n",
      "   0.          0.26666667]\n",
      " [48.          0.          0.         75.         47.         80.\n",
      "  88.          0.         76.          0.          0.         59.\n",
      "  57.         50.         92.          0.          4.          5.\n",
      "   0.43333333  0.          0.          0.51666667  0.28333333  0.5\n",
      "   0.91666667  0.        ]\n",
      " [ 0.         65.         73.          0.         88.         80.\n",
      "  84.         74.          0.         57.         63.          0.\n",
      "  55.         83.         48.         61.          2.          5.\n",
      "   0.          0.38333333  0.5         0.          0.60833333  0.775\n",
      "   0.51666667  0.54166667]\n",
      " [73.          0.         63.         63.          0.         94.\n",
      "  65.          0.         89.          0.         58.         77.\n",
      "   0.         67.         59.          0.          3.          3.\n",
      "   0.73333333  0.          0.39166667  0.55        0.          0.725\n",
      "   0.41666667  0.        ]\n",
      " [ 0.         70.          0.         64.         86.         79.\n",
      "  63.          0.          0.         45.          0.         46.\n",
      "  71.         86.         73.          0.          9.          0.\n",
      "   0.          0.44166667  0.          0.4         0.64166667  0.70833333\n",
      "   0.46666667  0.        ]\n",
      " [62.         71.         90.         78.          0.          0.\n",
      "  87.          0.         56.         79.         45.         53.\n",
      "   0.          0.         51.          0.          3.          6.\n",
      "   0.36666667  0.63333333  0.50833333  0.475       0.          0.\n",
      "   0.58333333  0.        ]\n",
      " [ 0.         87.          0.          0.         88.         49.\n",
      "  62.         77.          0.         42.          0.          0.\n",
      "  52.         53.         76.         80.          2.          6.\n",
      "   0.          0.44166667  0.          0.          0.6         0.28333333\n",
      "   0.58333333  0.74166667]\n",
      " [71.         43.         70.          0.         52.         53.\n",
      "  66.          0.         82.         57.         60.          0.\n",
      "  89.         67.         87.          0.          1.          4.\n",
      "   0.625       0.18333333  0.43333333  0.          0.575       0.4\n",
      "   0.675       0.        ]\n",
      " [ 0.         82.          0.          0.         81.         73.\n",
      "  87.         71.          0.         71.          0.          0.\n",
      "  45.         66.         71.         53.          9.          6.\n",
      "   0.          0.75833333  0.          0.          0.48333333  0.59166667\n",
      "   0.75        0.46666667]]\n"
     ]
    }
   ],
   "source": [
    "print(my_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.Input(shape=(input_size,), name=\"subjects\")\n",
    "# x = layers.Dense(20,activation='relu', name='Hidden1') (inputs)\n",
    "# x = layers.Dense(20,activation='relu', name='Hidden2') (x)\n",
    "# outputs = layers.Dense(output_size, activation=\"relu\", name=\"predictions\")(x)\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(input_size,), name=\"inputs\"))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(output_size, activation=\"relu\", name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = my_data[:,:input_size]\n",
    "\n",
    "x_train = (x_train - np.average(x_train, axis = 0))/np.std(x_train, axis = 0)\n",
    "\n",
    "y_train = my_data[:,input_size:]\n",
    "x_val = x_train[(int)((1-test_frac)*test_data_size):]\n",
    "y_val = y_train[(int)((1-test_frac)*test_data_size):]\n",
    "x_train = x_train[:(int)((1-test_frac)*test_data_size)]\n",
    "y_train = y_train[:(int)((1-test_frac)*test_data_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = keras.losses.mean_squared_error,\n",
    "    metrics = [keras.metrics.mean_squared_error]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3237 - val_mean_squared_error: 0.3237\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2791 - mean_squared_error: 0.2791 - val_loss: 0.2589 - val_mean_squared_error: 0.2589\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2282 - mean_squared_error: 0.2282 - val_loss: 0.2170 - val_mean_squared_error: 0.2170\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1946 - mean_squared_error: 0.1946 - val_loss: 0.1874 - val_mean_squared_error: 0.1874\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1712 - mean_squared_error: 0.1712 - val_loss: 0.1658 - val_mean_squared_error: 0.1658\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1538 - mean_squared_error: 0.1538 - val_loss: 0.1497 - val_mean_squared_error: 0.1497\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1395 - mean_squared_error: 0.1395 - val_loss: 0.1368 - val_mean_squared_error: 0.1368\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1278 - mean_squared_error: 0.1278 - val_loss: 0.1257 - val_mean_squared_error: 0.1257\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1177 - mean_squared_error: 0.1177 - val_loss: 0.1164 - val_mean_squared_error: 0.1164\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1088 - mean_squared_error: 0.1088 - val_loss: 0.1066 - val_mean_squared_error: 0.1066\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1005 - mean_squared_error: 0.1005 - val_loss: 0.0985 - val_mean_squared_error: 0.0985\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0926 - mean_squared_error: 0.0926 - val_loss: 0.0908 - val_mean_squared_error: 0.0908\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0846 - mean_squared_error: 0.0846 - val_loss: 0.0831 - val_mean_squared_error: 0.0831\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0774 - mean_squared_error: 0.0774 - val_loss: 0.0756 - val_mean_squared_error: 0.0756\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0711 - mean_squared_error: 0.0711 - val_loss: 0.0692 - val_mean_squared_error: 0.0692\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0658 - mean_squared_error: 0.0658 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0612 - mean_squared_error: 0.0612 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0575 - mean_squared_error: 0.0575 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0542 - mean_squared_error: 0.0542 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0514 - mean_squared_error: 0.0514 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0303 - val_mean_squared_error: 0.0303\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0274 - val_mean_squared_error: 0.0274\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0252 - val_mean_squared_error: 0.0252\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0237 - val_mean_squared_error: 0.0237\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0216 - val_mean_squared_error: 0.0216\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0200 - val_mean_squared_error: 0.0200\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0186 - val_mean_squared_error: 0.0186\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0148 - mean_squared_error: 0.0148 - val_loss: 0.0156 - val_mean_squared_error: 0.0156\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0143 - mean_squared_error: 0.0143 - val_loss: 0.0151 - val_mean_squared_error: 0.0151\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0134 - mean_squared_error: 0.0134 - val_loss: 0.0145 - val_mean_squared_error: 0.0145\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0135 - val_mean_squared_error: 0.0135\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0133 - val_mean_squared_error: 0.0133\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0128 - val_mean_squared_error: 0.0128\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0119 - val_mean_squared_error: 0.0119\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0117 - val_mean_squared_error: 0.0117\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_squared_error: 0.0100 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0095 - mean_squared_error: 0.0095 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0072 - val_mean_squared_error: 0.0072\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0069 - val_mean_squared_error: 0.0069\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history\n",
    "epochs = history.epoch\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist[\"mean_squared_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "    plt.show()  \n",
    "\n",
    "    print(\"Defined the plot_the_loss_curve function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_loss_curve function.\n"
     ]
    }
   ],
   "source": [
    "plot_the_loss_curve(epochs, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33972445 0.7058448  0.47569805 0.         0.60056055 0.6857699\n",
      " 0.5904247  0.        ]\n",
      "[0.4        0.675      0.525      0.         0.68333333 0.65833333\n",
      " 0.59166667 0.        ]\n",
      "[0.80014825 0.         0.32680786 0.71182406 0.836661   0.\n",
      " 0.         0.31261617]\n",
      "[0.75833333 0.         0.45833333 0.84166667 0.725      0.\n",
      " 0.         0.26666667]\n",
      "[0.4095822  0.         0.         0.52577066 0.49619555 0.5684635\n",
      " 0.7301319  0.        ]\n",
      "[0.43333333 0.         0.         0.51666667 0.28333333 0.5\n",
      " 0.91666667 0.        ]\n",
      "[0.         0.38508952 0.47155017 0.         0.49679327 0.7315061\n",
      " 0.42945513 0.66293865]\n",
      "[0.         0.38333333 0.5        0.         0.60833333 0.775\n",
      " 0.51666667 0.54166667]\n",
      "[0.63482815 0.         0.48090446 0.50399816 0.         0.6497169\n",
      " 0.60316545 0.        ]\n",
      "[0.73333333 0.         0.39166667 0.55       0.         0.725\n",
      " 0.41666667 0.        ]\n",
      "[0.         0.44542557 0.         0.49214795 0.63650274 0.69271076\n",
      " 0.48256725 0.        ]\n",
      "[0.         0.44166667 0.         0.4        0.64166667 0.70833333\n",
      " 0.46666667 0.        ]\n",
      "[0.49369568 0.65500396 0.62375575 0.5407733  0.         0.\n",
      " 0.64401317 0.        ]\n",
      "[0.36666667 0.63333333 0.50833333 0.475      0.         0.\n",
      " 0.58333333 0.        ]\n",
      "[0.         0.5286231  0.         0.         0.5966207  0.4030702\n",
      " 0.56704843 0.5637368 ]\n",
      "[0.         0.44166667 0.         0.         0.6        0.28333333\n",
      " 0.58333333 0.74166667]\n",
      "[0.60350925 0.13521925 0.47378528 0.         0.64254713 0.45307273\n",
      " 0.6497647  0.        ]\n",
      "[0.625      0.18333333 0.43333333 0.         0.575      0.4\n",
      " 0.675      0.        ]\n",
      "[0.         0.7375824  0.         0.         0.52044374 0.6478583\n",
      " 0.80326533 0.5096984 ]\n",
      "[0.         0.75833333 0.         0.         0.48333333 0.59166667\n",
      " 0.75       0.46666667]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_train[:10])\n",
    "for i in range(10):\n",
    "    print(predictions[i])\n",
    "    print(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"gradeModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leeji\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\leeji\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\leeji\\AppData\\Local\\Temp\\tmpolyz8ggt\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('gradeModel.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
